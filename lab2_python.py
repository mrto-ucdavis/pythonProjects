# -*- coding: utf-8 -*-
"""Lab2_python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wRIYjwbApwAQSRyzpT6LOGhtzfcSrbI1

# Experimental 2 Oscillating Piston
Developer: <b>Thomas To</b><sup>1-7 </sup> (Transfer Undergraduate, 2021) <br>
Created:   July 2020 <br>
In collaboration with Dr. Jiandi Wan<sup>1</sup>, Dr. Harishankar Manikantan<sup>1</sup> and Ashley Vater<sup>2</sup>
1. Department of Chemical Engineering, University of California, Davis
2. Genome Center, University of California, 
  Davis <br>
3. McNair Scholar, University of California, Davis
4. Avenue<sup>E</sup> Scholar, University of California, Davis
5. Student Outreach Ambassador, University of California, Davis
6. Undergraduate Research Ambassador, University of California, Davis
7. Laney Community College, Peralta District, Oakland

These templates are meant to help facilltate the big picture workflow of
the data-analysis and are designed to help getting you started. 
Ultimately, the ways in which you handle the data both in and out of the
laboratory will influence the effectiveness of these templates. <br>

<b> For more individualized support with coding or data handling, <i><u> don't
hesitate</u></i> to reach out to the (specific) tutors who have previously 
taken the course or otherwise, general (non-specific) tutors 
who can help suggest how to approach data handling in MATLAB or Python3. </b>

# Calibrating the Pressure Sensor
## For lab 2 (2.1 and 2.2) the experimental calibration value ***(mV / psi / Volt)*** is independent of the gas medium being meassured in the piston. The following template can be applied to the Air (Lab 2.1), Argon (Lab 2.2) and Carbon Dioxide (Lab 2.2) gases.

**In this lab, the template will be handling the data differently than in lab 1.** In this case, we will be taking advantage of the file exporting at the time of the experiment. At the time of this experiment when I took the class, my lab group decided to group each mass condition which contained the number of trials.

`Condition Folder(s) / Trial Folder(s) / .csv or .xlsx file from the sensor`

To make handling the data a bit more rational, I contained all of the conditions in a folder called, "Lab 2.1 Data" <br>
`Lab 2.1 Data / Condition Folder(s) / Trial Folder(s) / .csv or .xlsx file from the sensor`

We can take this a step further if you'd like and specify the, **"Lab 2.1 Data"** folder as **"Lab 2.1 Air Data"** since you'll be working with Argon and Carbon dioxide in Lab 2.2. <br>

In actuality, the data contained within the respected condition and trial folder looks like this:
```
For two conditions and two trials:
Condition 1 / Trial 1 Folder / .csv or .xlsx file from the sensor
Condition 1 / Trial 2 Folder / .csv or .xlsx file from the sensor

Condition 2 / Trial 1 Folder / .csv or .xlsx file from the sensor
Condition 2 / Trial 2 Folder / .csv or .xlsx file from the sensor
...
Condition i / Trial j Folder / .csv or .xlsx file from the sensor i,j
```
I'm purposefully not inlcuding brackets around the generalized indexes because this depends on how you handle the data. In the case of the python template, I personally liked using nested lists which indexes are contained as [  ].

One can see that using an iterative process using a nested for-loop to go through each condition and within each condition to go through each trial folder to access the data contained within the respected condition and trial.

Rationally, all you need to do using this template for Lab 2.1 and 2.2 (for Air, Argon and Carbon Dioxide) is to have the same naming convention for your condition folders and simply change your directory to access your Lab Data that might look something like this: <br>
`Lab 2.2 Data / Gas Type / Condition Folder(s) / Trial Folder(s) / .csv or .xlsx file from the sensor` <br>
So in actuality,
```
Lab 2.2 Data / CO2 / Condition Folder(s) / Trial Folder(s) / .csv or .xlsx file from the sensor
Lab 2.2 Data / Ar / Condition Folder(s) / Trial Folder(s) / .csv or .xlsx file from the sensor

If the name(s) of the condition folders are the same, you simply need to change the directory as needed to:
  Lab 2.2 Data / CO2 
  Lab 2.2 Data / Ar
```
Ideally you would have the same number of trials for each gas case. This is because, inorder to properly make a figure, the x and y axis must be the same size. If you do not; like my lab group did, I used the lowest trial number as the limiting number across all conditions but the template assumes the same number of trials since this was by design during the experiment.
"""

# If using google colab, you'll need to mount your drive inorder to access the files being stored in the directory
# from google.colab import drive
# drive.mount('/content/drive')

# Change directory as needed using %cd and %ls to check
# You should have your "Lab Data" Folder showing when using %ls

"""# Preparing data contained in .csv or .xslx files for ergonomic variable usage later on. Note: The procedure instructs to save as .csv 
## Determining the file path inorder to access the data of each trial per condition
At minimum, you should have four conditions: no mass, mass added, additional mass added and a folder containing static readings for calibration.<br>

*It is up to your lab group if you choose to have more, "additional mass added" conditions.*

The static reading condition folder looks different than the other conditions and my lab group handled the calibration data as such:<br>
```
  Lab 2.1 Data/ Static Pressure Measurements / 
   No mass Folder / .csv or .xlsx file from the sensor
   Mass 1 Added Folder / .csv or .xlsx file from the sensor
   Mass 1 + 2 Added Folder / .csv or .xlsx file from the sensor
   Mass 1 + 2 + 3 Added Folder / .csv or .xlsx file from the sensor
   Mass 1 + 2 + 3 + 4 Added Folder / .csv or .xlsx file from the sensor
   ...
   Lab 2.1 Data / Static Pressure Measurements / Mass i /.csv or .xlsx file from the sensor i
```
***You don't need to copy this verbatim, this is written to help you understand what's going on. Rationally, you can name these whatever you want since you'll see that the code template just needs access to the name of your condition folder, "Static Pressure Measurements" and will sort the mV readings for you.***

Don't be afraid to see your folder names looking like, "20200128_1234_2". What's more important is the condition folder name(s).
"""

#  Simultaneously importing data files (csv or excel) and storing in respective ergonomic vars
import os
import pandas as pd

# Calibration Data
volP = 'Lab 2.1 Data/Static Pressure Measurements/'  # Lab Data / Condition Data

# Dynamic Data
M1 = 'Lab 2.1 Data/Mass 1/' 
M12 = 'Lab 2.1 Data/Mass 1 + 2/'
noM = 'Lab 2.1 Data/Weightless/'

# Storing respected paths 
conditionpaths = [M1, M12, noM, volP] 
NUMCONDITIONS = len(conditionpaths)
# Generating nested lists to contain number of conditions
accessdata,storedata, storemV,storet=[[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)] 

filelist = [] # Stores trial values by condition dictated by path above
for paths in range(NUMCONDITIONS):
    for root, dirs, files in os.walk(conditionpaths[paths]):
      for file in files:
        if(file.endswith(".csv")): #.xlsx
          filelist.append([paths,os.path.join(root,file)]) 
          # tagging the "paths" index in the first nested list since this corresponds to the condition type

# Run through filelist and group by corresponding number to condiitons 
for i in range(len(filelist)): # run through
    for conditionnum in range(NUMCONDITIONS):
        #if index corresponds to condition, group it
        if filelist[i][0] == conditionnum: 
            storedata[conditionnum].extend([filelist[i][1]])
            #storedata[condition][trial] condition order dictated by conditionspath input

# Finds lowest trial number amongst conditions inorder to make figure properly
MINNUMTRIALS = []
for condition in range(NUMCONDITIONS):
    a = len(storedata[:][condition]) 
    MINNUMTRIALS.append(a)
NUMTRIALS = min(MINNUMTRIALS)

# Seperates each condition and trial's mV and time values to access data
for conditions in range(NUMCONDITIONS):
    for trials in range(NUMTRIALS):
        accessdata[conditions].append(pd.DataFrame(pd.read_csv(storedata[conditions][trials], skiprows=6)))
        storemV[conditions].append(accessdata[conditions][trials]["AI4 (mV)"])        
        # Number of rows correspond to time elapsed (ms or 0.001s)
        storet[conditions].append((accessdata[conditions][trials]["Sample"]))

"""# Preparing accessible data for Figure 1.
Checking to make sure all conditions have the same number of trials.
If not, will search for the lowest trial and use that value. 

Will sort experimental mV from lease to greatest corresponding to mass inputs
since mass inputs are dependent on how tape was handled. 
Careful attention should be paid to the sort order and unit handling of the 
conversion factor from the slope of the calibration curve; understanding 9V meaning.

Prints the mean and standard deviation in convienent scientific notation.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
# using VolP to calibrate a Voltage, Pressure calibration curve for unit converstion using it's slope
# from conditionpath, VolP is the 4th index, condition Index = 3 (since python starts at 0)

# Average each trial of this condition and note the standard deviation
# Sort from lowest to greatest
volP_avg = [[],[]] # mean, std; tagging trials index for sorted correlation
sortorder_mean = []
for trials in range(NUMTRIALS):
    sortorder_mean.append(np.mean(storemV[3][trials]))
    volP_avg[0].append(tuple([trials, np.mean(storemV[3][trials])])) #equivalent to zip
    volP_avg[1].append(tuple([trials, np.std(storemV[3][trials])]))

# Sort from lowest to highest values
sortorder_mean.sort()

# From sorted averaged values, corresponding trial number and standard deviation values sorted as well
# trial number sorting done incase calibration curve values not experimentally determined incrementally 
sortedMeanVals = [tuple for x in sortorder_mean for tuple in volP_avg[0] if tuple[:][1]== x] 
accessCorrespondingTrialNum = [trialorder[0] for trialorder in sortedMeanVals]

sortedStdVals = [tuple for x in accessCorrespondingTrialNum for tuple in volP_avg[1] if tuple[:][0]== x] 

accessMeanVal = [trialorder[1] for trialorder in sortedMeanVals]
accessStdVal = [trialorder[1] for trialorder in sortedStdVals]
# index corresponds to sorted lowest to highest values with corresponing trial number 
# stored in "accessCorrespondingTrialNum"

reportMeanVals = []
reportStdVals = []
for i in range(len(accessStdVal)):
    std_scientific_notation = "{:.2e}".format(accessStdVal[i])
    mean_scientific_notation = "{:.2e}".format(accessMeanVal[i])
    reportStdVals.append(std_scientific_notation)
    reportMeanVals.append(mean_scientific_notation)
#reportStdVals = testvar.append("{:.2e}".format(accessStdVal[i] for i in range(len(accessStdVal))))

print("The corresponding trial numbers for the mean and standard deviation is: \n" + str([accessCorrespondingTrialNum[i] + 1 for i in range(len(accessCorrespondingTrialNum))]))
print("The mean values are: \n" + str(reportMeanVals))
print("The corresponding std values are: \n" + str(reportStdVals))
# Note that the std values are of units, mV in which the error bars will be the vertical bars and not horizontal

"""# Figure 1: Calibration Curve
Now that we've prepared the data, you can now make the calibration curve to get the slope and to use this converstion factor knowing the experimental conditions.

The mass variable depends on how you handled the tape in determinnig the mass; whether you used a long peice of tape to secure all of the added masses; like my lab group did to make the computational work a bit easier or if you remeassured each time.

***The template assumes you used one peice of tape to secure all the weights throughout the calibration***
"""

# Diameter of inner tube: 
d_inner = []; # experimental measurements of the smaller tubes
d_inner_avg = np.mean(d_inner);
d_inner_std = np.std(d_inner);

# Provided Specifications from the Apparatus
D_piston = 32.56; # mm
g = 9.81; # m / s^2
D = D_piston/1000; # m

pi = np.pi # Ergonomical change

# Account for experimental mass of the tape and additional mass to piston during static phase
# The template assumes you used one peice of tape to secure all the weights throughout the calibration
# If you used a variable amount of tape to secure more of the mass or if you didn't use any tape,
#    you'll need to modify the template as needed.
TAPE = ; # g
PISTON = 35.0; # std = 0.5 g

# Mass of each independent weight added
WEIGHT = [0,]; # g; 

# experimental measurements of mass and tape depends on how tape was accounted for when performing experiment

m = WEIGHT[0] + PISTON + TAPE
WEIGHT[0] = m
### ASSUMES THE SAME TAPE BEING USED TO SECURE ALL WEIGHTS.
# RATIONALLY, WEIGHT IS ACCUMULATED OVER TIME:
# [No tape, 
#   Weight 1 + Tape,
#   Weight 1 + 2 + Tape,
# ...]
m = np.cumsum(WEIGHT)/1000 #Kg 

#  Pressure = Force / Contact Area = m * g / ( pi * D^2 / 4 )
calibrateX = m * ( 4 * g / (pi * np.power(D,2) ) ) / 6894.76 # Pa -> Psi
calibrateY = np.array(accessMeanVal) # In array format for use in "MATLAB" plot

m, b = np.polyfit(calibrateX,calibrateY, 1) # y = mx + b 
mVal = m/9 
plt.plot(calibrateX, calibrateY, 'o', label='Data Set')
# Good practice to include error bars however,
# if too small or not best represented in the figure, show in a tablular form.
plt.errorbar(
    calibrateX,
    calibrateY,
    yerr = accessStdVal # Size should be the same as number of corresponding points at (x,y)
)
# np.multiply does element wise multiplication
# %.3f limits the float value to 3 decimals; adjust as needed for significant figure reporting purposes
plt.plot(calibrateX, (np.multiply(m,calibrateX)+b), label='%.3f mV/psi/Volt'%mVal) 
plt.xlabel('Gauge pressure, psi');
plt.ylabel('Voltage, mV');
plt.legend()

"""# Preparing experimental data for non-linear curve fitting 
If all went well, this next section of the code is ready for you to run. The template code will output the various graphs for each condition and trial repsectively. Notice that values the physical meaning of higher pressures correspond to the compression of the piston. Therefore, higher pressures correspond to lowered piston positions. Thus, it makes the sense (*to me*) to start the plot at the lowest point in space-time which is at the highest pressure value, at the respected global max. 
# Accessing stored x and y values
You can access the x, y and timecorrection data using:<br>
  `storexval[conditionNum][trialsNum]`        
  `storeyval[conditionsNum][trialsNum]`        
Similar to previous labs, we can find the time at the max and implement a time correction. Taking a look at the code portion, the `timecorrection` to start the graph as well as unit conversion has already been applied. However, when working with data moving forward, you must take into account the timecorrection index values contained in `timecorrection[conditionsNum][trialsNum]`.
"""

# Experimental data implicitly are in units of mV/psi/9V therefore, we must multiply our "mV" data by 9 to have units, mV/psi for proper unit conversion 
storepsi = np.array(storemV)/(mVal*9) # All conditions, All trials contained in array are multiplied by constant, 9 of units, Voltage (V)

# The order of the condition name is depedent on how "conditionpaths" was defined initially
conditionName = ["Mass1", "Mass 1+2","No Mass"]
NUMCONDITIONS = len(conditionName) # New NUMCONDITION value moving forward

storemaxindex = timecorrection = [[] for _ in range(NUMCONDITIONS)]
storemaxpsival, startplot, storexval, storeyval = [[] for _ in range(NUMCONDITIONS)], [[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)]
for conditions in range(NUMCONDITIONS):
    for trials in range(NUMTRIALS):
        storemaxindex[conditions].append(np.argmax(storepsi[conditions][trials])) 
        startplot[conditions].append(storepsi[conditions][trials][storemaxindex[conditions][trials]:])
        xval = storet[conditions][trials][timecorrection[conditions][trials]:]
        xval = (xval-timecorrection[conditions][trials])/1E3
        yval = startplot[conditions][trials]
        storexval[conditions].append(xval)
        storeyval[conditions].append(yval)
        plt.plot(xval, yval,'o')
        
        # Remove title for use in report. 
        # This is written here to help qualitatively access your representative data
        # Also, this serves as sample code to help you continue qualitatively access parameter fitting values later
        plt.title("Condition: %s, Trial #: %i" %(conditionName[conditions],trials+1)) 
        plt.xlabel("Time, s")
        plt.ylabel("Gauge pressure, psi")
        plt.figure() # plots new figure and doesn't superimpose

"""# Theoretical Curve Fitting Parameters: A "Verbal" to Code Implementation
Determining Initial Guesses for Coefficients Based on, "fit2model.pdf"
## P0 : The pressure value given finite time $\rightarrow$ `yval[-1]`<br>
## C  : The difference between Pmax and P0 $\rightarrow$ `max(yval) - P0`<br>
## k : Solving k is a 3 step process, 
1. Determine `P-half` $\rightarrow$ `C/2` 
2. Determine t-half at P-half<br>
  In other words, determine `x` at `f(x)` $\rightarrow$
  `np.isclose(yval,Phalf,rtol=0.04, atol=0.004)`<br>
3. Use `np.log10()` ***NOT*** `np.log`<br>
np.log is the equivalent of using `ln` or the natural log. That's not what's used in the fit2model.pdf reference!

## $\omega$:  Identify and use the local minimum values *(which represent the highest point in space-time during oscillation)* and take the difference for each minima. Use the minima after some amount of time. 
```python
SAMPLE CODE FOR ONE ITERATION
from scipy.signal import argrelextrema 
minindex = argrelextrema(np.array(yval[conditions][trial]), np.less) # finds index of local mins 
mintimeindex = minindex+timecorrection[conditions][trial]
mintimeindex[0] # Forcing correction inorder to access data
# If iterating, would look like mintimeindex[conditions][trial][0]
perioddiff = [mintimeindex[0][i+1] - mintimeindex[0][i] for i in range(len(mintimeindex[0])-1)] # Forward Difference
minyvals = []
for i in range(len(minindex[0].tolist())):
    minyvals.append(yval.tolist()[minindex[0].tolist()[i]])
omegaval = pi/np.array(perioddiff)[2] # Remember in python, 2 is the 3rd value
```
The following code segement initiates nested lists to store the values of each fitting function parameter as such `[conditions][trials]` when using a for-loop of the structure:
```python
for conditions in range(NUMCONDITIONS):
  for trials in range(NUMTRIALS):
```
As well as, determines and stores the P0, C and P-half values for you. This will hopefully, help you be familiar with how to use the *single iterative sample code* into an interative process ***because once you know how to solve the problem once, iterating should be trivial***
"""

storeP0, storeC, storePhalf, storethalf, storek, storephi, storeomega = [[] for _ in range(NUMCONDITIONS)], [[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)], [[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)], [[] for _ in range(NUMCONDITIONS)],[[] for _ in range(NUMCONDITIONS)]

for conditions in range(NUMCONDITIONS):
  for trials in range(NUMTRIALS):
    storeP0[conditions].append(np.array(storeyval[conditions][trials])[-1])
    storeC[conditions].append(max(storeyval[conditions][trials]) - storeP0[conditions][trials])
    storePhalf[conditions].append(storeC[conditions][trials]/2)

"""# Find t-half for "params0(3)" from fit2model.pdf
## In python this is params0(2)
Note: fit2model.pdf is written in MATLAB<br>

Consider using `np.isclose(storeyval[conditions][trials],storePhalf[conditions][trials],rtol=0.04, atol=0.004)`<br>
*Reference: https://numpy.org/doc/stable/reference/generated/numpy.isclose.html*

This is equivalent to applying the "squeeze (or sandwhich) theorm" from calculus in which we would otherwise have to set up a loop that iterates through y-value index(es) that finds viable (True) experimental values that is below the upper threshold bound of `Phalf + upperBoundValue` but above the lower threshhold bound of `Phalf + lowerBoundValue`. <br>
The code would look something like this:
```python
SAMPLE CODE
for i in range(length(yval)):
  if (yval[i] < Phalf+upperBoundThrehold) & (yval[i] > Phalf+lowerBoundThreshhold):
    storeViablethalf.append[i/1000] # Since indexes conveniently correspond to units of ms, convert to s
```
Best case scenario, we can access the last value in storeViablethalf using `storeViablethalf[-1]`. If you have this iterating through conditions and trials, to obtain the last value, the variable would look something like, `storeViablethalf[conditions][trials][-1]`. However, If you apply this to all cases, you disregard any graphs that doesn't follow a typical decaying function.

***Remember this is experimental data, things could go wrong so it's up to you to consider when, where and how things can change both in and out of the laboratory. (Even what's said in these templates!)***

# Random hint that may or may not be relevant but is useful to know:
If `data` is an array, change into list form. In doing so, `~np.isnan` is applied and any non-NaN values in `data` are kept while all other nan values are removed and the side of `data` is reduced to contain only non-NaN values.
***How and why could this be useful?*** <br>

Remember when graphing, you need to have the same size x and y axis. From an original data set of x and y values, you can apply a sort of, "boolean sorting" which (similar to the sample code above), you would iterate through an axis and passing through a boolean condition statement, store `True` values if true and False otherwise (*else*). Then you would be able to graph/superimpose only values that were True (or False) and to cleanly access values contained in the "boolean sorted" variable, you would use `~np.isnan` where `False` values are assigned `np.nan()`<br>

I used this "logic" as a creative way of implementing "best practices" in qualitatively accessing each curve fitting parameter and to access how well each approximation is based on experimental data.
"""

# Your code here

"""# Ready for Theoretical Non-Linear Curve Fitting
Reference Documentation: <br>
https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html

https://towardsdatascience.com/basic-curve-fitting-of-scientific-data-with-python-9592244a2509
```python
# SAMPLE CODE: Works for single iteration
import scipy
def fittingFunction(t,P0,C,k,omega,phi): 
  return P0 + (C*np.exp(-k*t/2) * np.cos((omega*t)+phi)) 

pars, cov = scipy.optimize.curve_fit(
    f = fittingFunction,
    xdata = xval,
    ydata = yval,
    p0 =[P0, C, k, omega, phi]
)

plt.plot(xval, yval,'o',label='experimental') 
plt.plot(xval, fittingFunction(xval, *pars), linestyle='--',color='red',label='fit')
plt.xlabel('Time, s')
plt.ylabel('Gauge pressure, psi')
plt.legend() 
```
Up to you to figure out how to implement this sample code in an interative process to plot all of your conditions and trials; simiar to the 
**"Preparing experimental data for non-linear curve fitting"** section above which iteratively plots each condition and trial respectively
"""

# Your Code Here

"""# Exporting averaged and standard deviation values of fitting function parameters for each condition
For each fitting parameter of a condition, average the trial values; noting any anomalies and report tabular form in your write up. Recommended to export into tabular form (.csv or .xlsx) with reporting mean and std values for each condition
"""

# Your Code Here