# -*- coding: utf-8 -*-
"""Lab1_part1_python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XXYSJybbsi7N-RjxjeYnICGcYDfYZv9u

# Experimental 1 Thermocouples, Part 1 
Developer: <b>Thomas To</b><sup>1-7 </sup> (Transfer Undergraduate, 2021) <br>
Created:   July 2020 <br>
In collaboration with Dr. Jiandi Wan<sup>1</sup>, Dr. Harishankar Manikantan<sup>1</sup> and Ashley Vater<sup>2</sup>
1. Department of Chemical Engineering, University of California, Davis
2. Genome Center, University of California, 
  Davis <br>
3. McNair Scholar, University of California, Davis
4. Avenue<sup>E</sup> Scholar, University of California, Davis
5. Student Outreach Ambassador, University of California, Davis
6. Undergraduate Research Ambassador, University of California, Davis
7. Laney Community College, Peralta District, Oakland

#Hello and welcome to ECH 145A! 
The following and subsequent student
templates were developed to support students through the coding learning
curve. We hope through the facilitaiton of the computational aspect(s) 
of the course, students (like yourself!) can spend more time engaging
with chemical engineering concepts in a practical setting. <br>

You will have an option to use either MATLAB, Python(3) or Excel for use 
in data analysis. Traditionally, the ECH 145 course(s) were taught using
MATLAB exclusively. However, for the benefit of your long-term education,
we are putting efforts into adapating to Python3! <br>

Not only because Python is free but also because MATLAB is one of the
only programming langauges that starts it's indexing with 1 and can be
counter-intuitive when conceptually, initial times are defined at 0 time
units. (Also, semicolons)<br>

These templates are meant to help facilltate the big picture workflow of
the data-analysis and are designed to help getting you started. 
Ultimately, the ways in which you handle the data both in and out of the
laboratory will influence the effectiveness of these templates. <br>

<b> For more individualized support with coding or data handling, <i><u> don't
hesitate</u></i> to reach out to the (specific) tutors who have previously 
taken the course or otherwise, general (non-specific) tutors 
who can help suggest how to approach data handling in MATLAB or Python3. </b>

---

# Import your data 
Use `ls` to see what files are available. If you're using local Jupyter installed via Anaconda, your directory would have been manually determined. Use `cd` as needed to change directory in order to access files. <br>

**The way I liked to handle my data was to have a folder containing my data and this folder is contained in the same directory where you saved your `.ipynb` file. You'll see why and how useful this can be in the subsequent labs**

*For some reason, you need to use`%cd` or `%ls` in Google Colab to access your (mounted) google drive.*
"""

# Commented out IPython magic to ensure Python compatibility.
# %ls

"""# Check out your data, you might have Not-a-Number (NaN) values!
We need to get rid of NaN values inorder to
 quantify a mean (average) value and std (standard deviation) value.
 This can be done by using nanmean() from pandas <br>
 ```python
SAMPLE CODE:
import pandas as pd
# If using csv file,
# data = data = pd.read_csv('filename.csv') 
data = pd.read_excel('filename.xlsx') # I'm purposely keeping NaN values and not using keep_default_na = False
df = pd.DataFrame(data)

# Average trials with respect to the condition(s) of the experiment:
df_mean = np.nanmean(df[], axis = 0) # axis = 0 calculates column wise

# You're going to have two sets of data (Temperature, Voltage) at three temperature conditions (Hot, Ambient, Cold) for two equipment conditions (Commercial or Homemade)

# Consider the var: T_H_C for Temperature, Hot, Commerical
# This naming convention will contain the averaged trials for use in plotting Figure 1.
# T_H_C V_H_C   T_H_O V_H_O
# T_A_C V_A_C   T_A_O V_A_O
# T_C_C V_C_C   T_C_O V_C_O
 ```
---
At this point in the workflow you should be taking the average of each
trial per condition, we still need to take the average of the averages!
This is one way of handling the data because in my mind, it makes more
sense when determining the standard deviation (to two standard deviations)
of the averages between conditions than the trials themselves 
but this is up to you! <br>

Remember, you're going to have both Voltage and Temperature values.
Your <b>x-axis is in units of Celcius</b> and your <b>y-axis is in mV.</b><br>
<b> Double check what units your average values are in and adjust accordingly</b>

---
"""

# Ideally you would "run all" in Runtime when you first run your code after opening it
# As you progress through each coding section in Jupyter, the variables will be saved as well as your imported libraries.
import pandas as pd 
import numpy as np

# Importing Data Using Sample Code Above as Reference

# Sort out conditions and trials then average within the trials by conditions

# Take the average of each respective condition for best fitline

# Consider containing your conditions in a (tuple) variable to help keep things organized
# SAMPLE CODE:
# T_O = (T_H_O, T_A_O, T_C_O) 
# V_C = (V_H_C, V_A_C, V_C_C)

"""# Determining standard deviation two standard deviations (2$\sigma$) by using 2*np.std()

If you think mentioning the standard deviation within the trials is 
important in your write up, then mention it! <br>

Don't let this template stop you from being curious but question what the
significance of that would be and what the meaning of doing so is! By taking the standard deviaton of the trials themselves within the
condition, you're analyzing the distribtuion (from the mean) in the
particular trial you're using.<br>

```python
 SAMPLE CODE:
 import numpy as np
 # Since we have three conditions, hot, ambient and cold:
 std_T_O = [] # Contains T_H_O, T_A_O, T_C_O
 std_V_C = [] # Contains V_H_C, V_A_C, V_C_C
 conditionNum = 3
 
 # Standard Deviations to 2-sigma
 for i = in range(conditionNum):
  std_T_O.append(2*np.std(T_O[i]))
  std_V_C.append(2*np.std(V_C[i]))

```
"""

# Your Code Here

"""# Great work! We're all ready to make the figure!
Throughout ECH 145A and ECH 145B you will develop an understanding between experimental and theoreteical data sets. Your experimental data set(s) will be represented as a scatter plot whereas your theoretical data set(s) will be plots that overlay ontop of the experimental scatter plot to assess how well the theoretical fit is compared to experimental and/or vice versa.

Again remember, your <b>x-axis is in units of Celcius</b> and your <b>y-axis is in mV.</b><br>
<b> Double check what units your average values are in and adjust accordingly</b>

---

To begin, we must first import a library to visualize our data in python:
`import matplotlib.pyplot as plt`
For those who may not be as familiar with python and MATLAB, the use of `[ ]` in either language means different things. In a way, the `plt` library is "pseudo-MATLAB". By this I mean, in order to visualize our data, we must first contain our data values in an array with the caveaut that Python3 does not have arrays, python has lists.

Use `np.array()` to "redefine" the list to an array. <br>
If you'd like, you can export the figure using `plt.savefig("filename.png")`


```python
SAMPLE CODE:
import numpy as np
import matplotlib.pyplot as plt
# Preparing Experimental Data
xval = np.array()
yval = np.array()

# Linear Best Fit
m, b = np.polyfit(xval, yval, 1)

# Determining Size of Error Bars
xerr = std.condition1_x
yerr = std.condition1_y

# Setting Up Subplots (2 row, 1 columns)
fig, ax = plt.subplots(2, 1)
ax[0].errorbar(xval, yval, # locations
            xerr=xerr,  #x-bar sizes (standard deviations?)
            yerr=yerr,  #y-bar sizes
            fmt='none')   #fmt='none' draw error bars without data markers otherwise style 'ro' etc
           
# Add label respectively for graph so legend can show automatically
ax[0].plot(xval,yval, 'ro', label='Homemade') 
ax[0].plot(xval, m*xval+b, 'k', label='Seebeck Coefficient: %.3f mV/$^\circ$C' % (m))
ax[0].legend(loc="lower right")

# Plotting second sub-plot
ax[1].plot(xval2,yval2, 'go', label='Commerical')
ax[1].plot(xval2, m2*xval2+b2, c = 'purple', label='Seebeck Coefficient: %.3f mV/$^\circ$C' % (m2)) 
ax[1].errorbar(xval2, yval2, # locations
            xerr=xerr2,  
            yerr=yerr2,  
            fmt='none')   #fmt='none' draw error bars without data markers            
ax[1].legend(loc="lower right")

plt.xlabel('Temperature, $^\circ$C');
plt.ylabel('Measured Voltage, mV');
#plt.show() This is necessary if using local jupyter
```
"""

import numpy as np
import matplotlib.pyplot as plt

plt.xlabel('Temperature, $^\circ$C');
plt.ylabel('Measured Voltage, mV');

"""# Calculation 1: Determining the relative Seebeck Coefficient
From the previous section, you should know the slope of the homemade and commerically available thermocouple:<br>

Using a (reliable?) reference found online, determine the relative Seebeck coefficient for the type and material of thermocouple used. <br>
*(what fun would it be if I gave you what I used here?)*

```
SAMPLE CALCULATION:
((experimentalSeebeck - theoreticalSeebeck )/theoreticalSeebeck) * 100
Remember, you want to know the error values by two standard deviations for this calculation
```
"""



"""# Calculation 1: Data Representation 1 <br>
<i>"Determine the temperature
difference measured for laboratory-constructed thermocouple
probes and commercially supplied thermocouple probes. This
comparison should be presented in a clear and intuitively
obvious way."</i>

I didn't understand what this meant when I read it when I went through 
the class so I did what was most intuitive to me... so be creative! <br>
*(Yes, **<u>i'm not going to tell you what I did </u>**because that'll take away from 
your creativity and intuitive way of thinking!)*
"""

